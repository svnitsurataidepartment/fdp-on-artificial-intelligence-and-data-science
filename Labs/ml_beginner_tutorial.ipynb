{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Beginner Tutorial\n",
    "## TensorFlow & scikit-learn\n",
    "\n",
    "This notebook introduces two essential Python libraries for machine learning:\n",
    "- **scikit-learn**: Traditional ML algorithms (classification, regression, clustering)\n",
    "- **TensorFlow**: Deep learning and neural networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if needed (uncomment to run)\n",
    "!pip install tensorflow scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART A: scikit-learn Tutorial\n",
    "---\n",
    "\n",
    "scikit-learn provides simple and efficient tools for:\n",
    "- Classification\n",
    "- Regression\n",
    "- Clustering\n",
    "- Dimensionality reduction\n",
    "- Model selection and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data\n",
    "\n",
    "scikit-learn includes several built-in datasets for practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_wine, make_classification, make_regression\n",
    "\n",
    "# Load the famous Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# X = features (measurements), y = target (species)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"Target names: {iris.target_names}\")\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier viewing\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = [iris.target_names[i] for i in y]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split\n",
    "\n",
    "Always split your data into training and testing sets to evaluate model performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42,    # For reproducibility\n",
    "    stratify=y          # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Most ML algorithms perform better when features are scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# StandardScaler: transforms data to have mean=0 and std=1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data, transform both train and test\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # fit + transform\n",
    "X_test_scaled = scaler.transform(X_test)        # transform only (use training stats)\n",
    "\n",
    "print(\"Before scaling:\")\n",
    "print(f\"  Mean: {X_train.mean(axis=0).round(2)}\")\n",
    "print(f\"  Std: {X_train.std(axis=0).round(2)}\")\n",
    "\n",
    "print(\"\\nAfter scaling:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean(axis=0).round(2)}\")\n",
    "print(f\"  Std: {X_train_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Models\n",
    "\n",
    "scikit-learn follows a consistent API:\n",
    "1. Create the model\n",
    "2. `fit()` on training data\n",
    "3. `predict()` on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create and train model\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.2%}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create and train\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X_train, y_train)  # Trees don't require scaling\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_tree):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.2%}\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "for name, importance in zip(iris.feature_names, rf.feature_importances_):\n",
    "    print(f\"  {name}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# KNN requires scaled data\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, y_pred_knn):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM requires scaled data\n",
    "svm = SVC(kernel='rbf', C=1.0)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression Example\n",
    "\n",
    "Regression predicts continuous values instead of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create synthetic regression data\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=3, noise=10, random_state=42)\n",
    "\n",
    "# Split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predict\n",
    "y_pred_reg = lin_reg.predict(X_test_reg)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"Linear Regression Results:\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  RÂ² Score: {r2:.4f}\")\n",
    "print(f\"  Coefficients: {lin_reg.coef_.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation\n",
    "\n",
    "Cross-validation gives a more robust estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 5-fold cross-validation\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf')\n",
    "}\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results:\\n\")\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Scores: {scores.round(3)}\")\n",
    "    print(f\"  Mean: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Get predictions from best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Clustering (Unsupervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_train_scaled)\n",
    "\n",
    "# Evaluate clustering\n",
    "silhouette = silhouette_score(X_train_scaled, cluster_labels)\n",
    "print(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "print(f\"Cluster centers shape: {kmeans.cluster_centers_.shape}\")\n",
    "\n",
    "# Visualize clusters (using first 2 features)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "            c='red', marker='X', s=200, label='Centroids')\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, cmap='viridis')\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.title('True Labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pipelines\n",
    "\n",
    "Pipelines chain preprocessing and modeling steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "# Use like a regular model\n",
    "pipeline.fit(X_train, y_train)\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Pipeline Accuracy: {accuracy:.2%}\")\n",
    "print(\"\\nPipeline steps:\")\n",
    "for name, step in pipeline.named_steps.items():\n",
    "    print(f\"  {name}: {step.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART B: TensorFlow Tutorial\n",
    "---\n",
    "\n",
    "TensorFlow is a deep learning framework for:\n",
    "- Neural networks\n",
    "- Image recognition\n",
    "- Natural language processing\n",
    "- And much more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TensorFlow Basics - Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors are the fundamental data structure\n",
    "\n",
    "# Scalar (0-D tensor)\n",
    "scalar = tf.constant(42)\n",
    "print(f\"Scalar: {scalar}, shape: {scalar.shape}\")\n",
    "\n",
    "# Vector (1-D tensor)\n",
    "vector = tf.constant([1, 2, 3, 4, 5])\n",
    "print(f\"Vector: {vector}, shape: {vector.shape}\")\n",
    "\n",
    "# Matrix (2-D tensor)\n",
    "matrix = tf.constant([[1, 2], [3, 4], [5, 6]])\n",
    "print(f\"Matrix shape: {matrix.shape}\")\n",
    "\n",
    "# 3-D tensor\n",
    "tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "print(f\"3D Tensor shape: {tensor_3d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor operations\n",
    "a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "\n",
    "print(\"Element-wise addition:\")\n",
    "print(tf.add(a, b).numpy())\n",
    "\n",
    "print(\"\\nMatrix multiplication:\")\n",
    "print(tf.matmul(a, b).numpy())\n",
    "\n",
    "print(\"\\nElement-wise multiplication:\")\n",
    "print(tf.multiply(a, b).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network for Classification (Iris Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for neural network\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# One-hot encode the labels\n",
    "lb = LabelBinarizer()\n",
    "y_train_onehot = lb.fit_transform(y_train)\n",
    "y_test_onehot = lb.transform(y_test)\n",
    "\n",
    "print(f\"Original label: {y_train[0]}\")\n",
    "print(f\"One-hot encoded: {y_train_onehot[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Sequential model\n",
    "model = keras.Sequential([\n",
    "    # Input layer (4 features)\n",
    "    layers.Input(shape=(4,)),\n",
    "    \n",
    "    # Hidden layer 1\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),  # Regularization\n",
    "    \n",
    "    # Hidden layer 2\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    # Output layer (3 classes)\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# View model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_onehot,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=0  # Quiet training\n",
    ")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Time')\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Time')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_onehot, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Classification with MNIST\n",
    "\n",
    "The MNIST dataset contains 70,000 grayscale images of handwritten digits (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f\"Training images: {X_train_mnist.shape}\")\n",
    "print(f\"Test images: {X_test_mnist.shape}\")\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train_mnist[i], cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_train_mnist[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample MNIST Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train_mnist = X_train_mnist.astype('float32') / 255.0\n",
    "X_test_mnist = X_test_mnist.astype('float32') / 255.0\n",
    "\n",
    "# Reshape for CNN: (samples, height, width, channels)\n",
    "X_train_mnist = X_train_mnist.reshape(-1, 28, 28, 1)\n",
    "X_test_mnist = X_test_mnist.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"Reshaped training data: {X_train_mnist.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Convolutional Neural Network (CNN)\n",
    "cnn_model = keras.Sequential([\n",
    "    # Input\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    \n",
    "    # Convolutional block 1\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional block 2\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten and dense layers\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    # Output layer (10 digits)\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # Use with integer labels\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train_mnist, y_train_mnist,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test_mnist, y_test_mnist, verbose=0)\n",
    "print(f\"\\nCNN Test Accuracy: {test_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and visualize\n",
    "predictions = cnn_model.predict(X_test_mnist[:10], verbose=0)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_test_mnist[i].reshape(28, 28), cmap='gray')\n",
    "    color = 'green' if predicted_labels[i] == y_test_mnist[i] else 'red'\n",
    "    ax.set_title(f\"Pred: {predicted_labels[i]} (True: {y_test_mnist[i]})\", color=color)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('CNN Predictions on Test Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building Models with Functional API\n",
    "\n",
    "The Functional API allows more complex architectures (multiple inputs/outputs, skip connections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API example\n",
    "inputs = keras.Input(shape=(4,), name='input_layer')\n",
    "\n",
    "x = layers.Dense(64, activation='relu', name='hidden_1')(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Dense(32, activation='relu', name='hidden_2')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "outputs = layers.Dense(3, activation='softmax', name='output_layer')(x)\n",
    "\n",
    "functional_model = keras.Model(inputs=inputs, outputs=outputs, name='functional_model')\n",
    "functional_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Callbacks\n",
    "\n",
    "Callbacks let you customize training behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping - stop training when validation loss stops improving\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Reduce learning rate when loss plateaus\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Model checkpoint - save best model\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "print(\"Callbacks defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with callbacks\n",
    "functional_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = functional_model.fit(\n",
    "    X_train_scaled, y_train_onehot,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Training stopped after {len(history.history['loss'])} epochs\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "np.random.seed(42)\n",
    "X_nn = np.random.rand(1000, 5)\n",
    "y_nn = 3 * X_nn[:, 0] + 2 * X_nn[:, 1] - X_nn[:, 2] + np.random.randn(1000) * 0.1\n",
    "\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(\n",
    "    X_nn, y_nn, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build regression model\n",
    "reg_model = keras.Sequential([\n",
    "    layers.Input(shape=(5,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # No activation for regression\n",
    "])\n",
    "\n",
    "reg_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Train\n",
    "reg_history = reg_model.fit(\n",
    "    X_train_nn, y_train_nn,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_mse, test_mae = reg_model.evaluate(X_test_nn, y_test_nn, verbose=0)\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "cnn_model.save('mnist_cnn_model.keras')\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model('mnist_cnn_model.keras')\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Verify it works\n",
    "test_loss, test_acc = loaded_model.evaluate(X_test_mnist[:100], y_test_mnist[:100], verbose=0)\n",
    "print(f\"Loaded model test accuracy: {test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Training Loop (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training gives you full control\n",
    "\n",
    "# Create a simple model\n",
    "custom_model = keras.Sequential([\n",
    "    layers.Input(shape=(4,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Custom training step\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = custom_model(x, training=True)\n",
    "        loss = loss_fn(y, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, custom_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, custom_model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# Convert to tensors\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train_scaled.astype('float32'), y_train_onehot.astype('float32'))\n",
    ").shuffle(1000).batch(batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "predictions = custom_model(X_test_scaled.astype('float32'))\n",
    "accuracy = np.mean(np.argmax(predictions, axis=1) == y_test)\n",
    "print(f\"\\nCustom training accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary: Quick Reference\n",
    "---\n",
    "\n",
    "## scikit-learn Workflow\n",
    "```python\n",
    "# 1. Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Create and train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3. Predict and evaluate\n",
    "predictions = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "```\n",
    "\n",
    "## TensorFlow/Keras Workflow\n",
    "```python\n",
    "# 1. Build model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# 2. Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Train\n",
    "model.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "# 4. Evaluate\n",
    "model.evaluate(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts Covered\n",
    "\n",
    "### scikit-learn:\n",
    "- Loading datasets\n",
    "- Train/test splitting\n",
    "- Data preprocessing (scaling)\n",
    "- Classification models (Logistic Regression, Decision Trees, Random Forest, KNN, SVM)\n",
    "- Regression models\n",
    "- Cross-validation\n",
    "- Hyperparameter tuning (GridSearchCV)\n",
    "- Clustering (K-Means)\n",
    "- Pipelines\n",
    "\n",
    "### TensorFlow/Keras:\n",
    "- Tensors and operations\n",
    "- Sequential API\n",
    "- Functional API\n",
    "- Convolutional Neural Networks (CNNs)\n",
    "- Callbacks (Early Stopping, Learning Rate Scheduling)\n",
    "- Saving and loading models\n",
    "- Custom training loops\n",
    "\n",
    "---\n",
    "**Happy Learning!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
