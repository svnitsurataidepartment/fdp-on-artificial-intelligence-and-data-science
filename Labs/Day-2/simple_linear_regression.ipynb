{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15f88657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /data/miniconda/envs/my_env/lib/python3.12/site-packages (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "# Simple Linear Regression Using OLS and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "100b86ac-3fe3-4032-9d29-8b3fc983ec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /data/miniconda/envs/my_env/lib/python3.12/site-packages (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19917d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f82ed1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple linear equation: y = 0.50 + 0.84x\n",
      "Slope (Coefficient): 0.8399999999999999\n",
      "Intercept: 0.5000000000000004\n",
      "Prediction for x=6: 5.54\n"
     ]
    }
   ],
   "source": [
    "# Find the parameters using the Ordinary Least Squares (OLS) Method\n",
    "# SimpleLinearRegression involves only a single feature (x) and the target value is a linear estimator\n",
    "# y_pred = w0 + w1 * x\n",
    "class SimpleLinearRegression:\n",
    "    def __init__(self):\n",
    "        self.slope = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the slope (w1) and intercept (w0) using the least squares method.\n",
    "        x and y should be 1-dimensional arrays/lists.\n",
    "        \"\"\"\n",
    "        # Calculate the mean of the input (x) and output data (y)\n",
    "        x_mean = np.mean(x)\n",
    "        y_mean = np.mean(y)\n",
    "\n",
    "        # Calculate the terms needed for the slope and intercept using the formulae that we derived\n",
    "        numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "        denominator = np.sum((x - x_mean) ** 2)\n",
    "\n",
    "        # Calculate the slope (w1) and intercept (w0)\n",
    "        self.slope = numerator / denominator\n",
    "        self.intercept = y_mean - self.slope * x_mean\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Makes predictions using the calculated slope and intercept or in common terms the weights (y = w0 + w1*x).\n",
    "        \"\"\"\n",
    "        if self.slope is None or self.intercept is None:\n",
    "            raise Exception(\"Model not trained yet. Call fit() first.\")\n",
    "        return self.intercept + self.slope * x\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Sample data: e.g., Years of Experience vs. Salary (in lakhs per annum)\n",
    "x_data = np.array([1, 2, 3, 4, 5])\n",
    "y_data = np.array([0.9, 2.5, 3.6, 3.5, 4.6])\n",
    "\n",
    "# Create and train the model\n",
    "model = SimpleLinearRegression()\n",
    "model.fit(x_data, y_data)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Simple linear equation: y = {model.intercept:.2f} + {model.slope:.2f}x\") # Correction for variable names\n",
    "print(f\"Slope (Coefficient): {model.slope}\")\n",
    "print(f\"Intercept: {model.intercept}\")\n",
    "\n",
    "# Make a prediction for a new value (e.g., 6 years of experience)\n",
    "new_x = np.array([6])\n",
    "prediction = model.predict(new_x)\n",
    "print(f\"Prediction for x={new_x[0]}: {prediction[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56518f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f571e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.34 2.18 3.02 3.86 4.7 ]\n"
     ]
    }
   ],
   "source": [
    "# Measure the mean squared error for the whole training data\n",
    "# usually it is done on a validation data where the actual target values are present\n",
    "pred_y_data = model.predict(x_data)\n",
    "# the above finds the predicted values on the input x_data\n",
    "# Print the predicted y values\n",
    "print(pred_y_data)\n",
    "mean_sq_error_y = mean_squared_error(y_data, pred_y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd6b104c-4602-426b-8a01-3856acfb6623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15440000000000004"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_sq_error_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bef5551-5595-4aaa-b134-ed4b30e458b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c9d9c82-a548-46ec-808a-125bf3277d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16ff043e-9cfd-495e-9529-04652064f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of the parameters\n",
    "# let us assume w0 = 0, w1 = 0 (any initialization will work, but the algorithm will not converge)\n",
    "w0, w1 = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f3d4b8c-f69f-4517-8b4b-a68ae3d02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate or step size\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3b68f31-5842-4358-bc19-b039505863ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "[0.9 2.5 3.6 3.5 4.6]\n",
      "[2.28709235 3.50888415 4.73067596 5.95246776 7.17425956]\n",
      "3.3723421863806697\n"
     ]
    }
   ],
   "source": [
    "# Apply gradient descent\n",
    "# Gradient descent is dependent on the loss function that we want to minimize\n",
    "# we generally use Mean Squared Error (MSE) or Sum of Squared Error (SSE)\n",
    "# Let us take the Sum of Squared Error (SSE)\n",
    "# L = sum_{i=1 to n} (y_i - y_i_pred) ^ 2, apply gradient descent as\n",
    "# w_new = w_old - lr * ∂L/∂w\n",
    "# there are two ways to end the gradient descent algorithm\n",
    "# 1. Upto maximum iterations 2. Until convergence (when the parameters do not change)\n",
    "# Let us take batch training\n",
    "max_iterations = 100 # if the number of iteration is very high, then the solution will diverge\n",
    "w0, w1 = 1, 1 # initializing to 0 will have issues, figure that out\n",
    "# you can randomly initialize\n",
    "# w0, w1 = np.random.random(2)\n",
    "print(w0, w1)\n",
    "def batch_gradient_descent_with_max_iterations(w0, w1, max_iterations):\n",
    "    \"\"\"Find the parameters using the maximum iterations.\"\"\"\n",
    "    for i in range(max_iterations):\n",
    "        total_update_in_w0 = 0\n",
    "        total_update_in_w1 = 0\n",
    "        for index in range(y_data.shape[0]):\n",
    "            pred_y_data = w0 + w1 * x_data[index]\n",
    "            update_in_w0 = y_data[index] - pred_y_data\n",
    "            update_in_w1 = (y_data[index] - pred_y_data) * x_data[index]\n",
    "            total_update_in_w0 += update_in_w0\n",
    "            total_update_in_w1 += update_in_w1\n",
    "        w0 -= lr * total_update_in_w0  # collect updates from all samples and update the parameters\n",
    "        w1 -= lr * total_update_in_w1\n",
    "    return w0, w1\n",
    "\n",
    "def predict_target_values(w0, w1, x):\n",
    "    \"\"\"Predict target values using a linear regression model or weights.\"\"\"\n",
    "    predicted_targets = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        predicted_targets[i] = w0 + w1 * x[i]\n",
    "    return predicted_targets\n",
    "\n",
    "w0, w1 = batch_gradient_descent_with_max_iterations(w0, w1, max_iterations)\n",
    "predicted_targets_using_batch_gd = predict_target_values(w0, w1, x_data)\n",
    "print(y_data)\n",
    "print(predicted_targets_using_batch_gd)\n",
    "mean_squared_error_batch_gd = mean_squared_error(y_data, predicted_targets_using_batch_gd)\n",
    "print(mean_squared_error_batch_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b81b83c-fb82-4efc-9563-922511653660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent_till_convergence(w0, w1):\n",
    "    \"\"\"Find the parameters till convergence i.e. parameters do not change or the change in negligible.\"\"\"\n",
    "    prev_w0, prev_w1 = w0, w1\n",
    "    tolerance = 0.001\n",
    "    for i in range(max_iterations):\n",
    "        total_update_in_w0 = 0\n",
    "        total_update_in_w1 = 0\n",
    "        for index in range(y_data.shape[0]):\n",
    "            pred_y_data = w0 + w1 * x_data[index]\n",
    "            update_in_w0 = y_data[index] - pred_y_data\n",
    "            update_in_w1 = (y_data[index] - pred_y_data) * x_data[index]\n",
    "            total_update_in_w0 += update_in_w0\n",
    "            total_update_in_w1 += update_in_w1\n",
    "        w0 -= lr * total_update_in_w0  # collect updates from all samples and update the parameters\n",
    "        w1 -= lr * total_update_in_w1\n",
    "        if abs(w0 - prev_w0) <= tolerance and abs(w1 - prev_w1) <= tolerance:\n",
    "            break\n",
    "        else:\n",
    "            prev_w0, prev_w1 = w0, w1\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35933b84-3aea-4a73-b6c2-91e051d63468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9 2.5 3.6 3.5 4.6]\n",
      "[2.28709235 3.50888415 4.73067596 5.95246776 7.17425956]\n",
      "3.3723421863806697\n"
     ]
    }
   ],
   "source": [
    "w0, w1 = 1, 1\n",
    "w0, w1 = batch_gradient_descent_till_convergence(w0, w1)\n",
    "predicted_targets_using_batch_gd_converged = predict_target_values(w0, w1, x_data)\n",
    "print(y_data)\n",
    "print(predicted_targets_using_batch_gd_converged)\n",
    "mean_squared_error_batch_gd_converged = mean_squared_error(y_data, predicted_targets_using_batch_gd_converged)\n",
    "print(mean_squared_error_batch_gd_converged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182a8cc-aa59-4d84-9a3d-e956b529b4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
